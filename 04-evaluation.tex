\section{Evaluation}

This section presents an in-depth empirical analysis of the design discussed in
Section~\ref{design-section}. To this end, we assess the scalability and
efficiency of the solution using multiple criteria.
%
First, an evaluation of the strong scaling behavior of the distributed
algorithm is presented.
%
Second, the weak scaling of the solution is assessed with respect to increasing
numbers of targeted latent communities.
%
Next, the effects of varying the number of latent communities on the system's
performance is analyzed.
%
Further, we evaluate the efficiency and overhead associated with network
communication between cluster nodes.
%
Finally, we contrast the effectiveness of scaling the computation horizontally
and vertically.

\begin{table*}
  \centering
  \begin{tabular}{l r r l}
    Name            & \#Vertices &       \#Edges & Description \\
    \hline
    com-LiveJournal &  3,997,962 &    34,681,189 & Online blogging social network \\
    com-Friendster  & 65,608,366 & 1,806,067,135 & Online gaming social network \\
    com-Orkut       &  3,072,441 &   117,185,083 & Online social network \\
    com-Youtube     &  1,134,890 &     2,987,624 & Video-sharing social network \\
    com-DBLP        &    317,080 &     1,049,866 & Computer science bibliography collaboration network \\
    com-Amazon      &    334,863 &       925,872 & Product co-purchasing network \\
    \hline
  \end{tabular}
  \label{tab-datasets}
  \caption{Summary of SNAP graph data sets used for evaluation.}
\end{table*}

Empirical results were obtained by performing experiments on the VU and TU/D
DAS5 clusters which consist of 68 and 48 compute nodes respectively. Each
compute node is equipped with a dual 8-core Intel Xeon E5-2630v3 CPU clocked
at 2.40GHz, 64GB of memory and 8TB of storage. Moreover, the compute nodes of
each site are interconnected by an internal 1~Gbit/s Ethernet and
FDR~InfiniBand. All of the experiments reported in this section utilized
publicly available graphs from the Stanford Large Network Dataset
Collection~(SNAP) \cite{snapnets}. Table~\ref{tab-datasets} lists the
collection of data sets used.

%################################################
\subsection{Strong scaling}

\begin{figure*}[t] % [htb]
  \centering
  \epsfig{file=plots/sweep-over-np-fixed-K.eps, width=\textwidth}
  \caption{Strong scaling: Execution time of 2048 algorithm iterations for the
  same problem size (com-Friendster, K=1024, M=16384 n=32) across different
  cluster sizes.}
  \label{fig-strong-scaling}
\end{figure*}

In order to evaluate the horizontal scalability of the distributed
implementation we tested the system's performance across different cluster
sizes while holding the problem size constant. For this study, we used the
com-Friendster graph as it contains the largest number of vertices and edges.
%
Figure~\ref{fig-strong-scaling}-a presents the execution time of 2048 algorithm
iterations across multiple cluster sizes. As shown in the figure, the execution
time steadily decreases by increasing the cluster size.
A deeper analysis of the individual execution phases of the algorithm provides
insights into the scalability if its building blocks.
%
In addition to the total execution time, Figure~\ref{fig-strong-scaling}-a
presents the cumulative time spent in individual computational phases across
iterations. Moreover, Figure~\ref{fig-strong-scaling}-b presents the speedup
achieved for the same experiments reported in
Figure~\ref{fig-string-scaling}-a. As is clearly shown, the dominant phase of
the execution is update\_pi.
The reported total time for each cluster size is significantly less than the
sum of the execution times of the individual phases. This is due to the
overlapping execution of the two most expensive phases, namely, update\_pi and
deploy\_minibatch. Both of these phases initially gain significant speedup with the
addition of compute nodes. However, the speedup curve gradually slows down for
larger cluster sizes as the work granularity of each worker node decreases
limiting their resource utilization.
%
The time spent in update\_beta remains relatively constant across cluster
sizes as it is an insignificant amount of work compared to the synchronization
overhead required to perform a collective MPI reduction operation to complete
it.

%################################################
\subsection{Weak Scaling}

\begin{figure}[t] % [htb]
  \centering
  \epsfig{file=plots/sweep-over-K-proportional-np.eps, width=\columnwidth}
  \caption{Weak scaling: varying the number of compute nodes proportionally to
  the number of latent communities.}
  \label{fig-weak-scaling}
\end{figure}

\subsection{Effects of Number of Communities on Performance}
\begin{figure}[t] % [htb]
  \centering
  \epsfig{file=plots/sweep-over-K-fixed-np.eps, width=\columnwidth}
  \caption{Effects of varying the number of communities on the algorithm's
  throughput.}
  \label{fig-throughput}
\end{figure}

\subsection{Horizontal vs. Vertical Scalability}
\begin{figure}[t] % [htb]
  \centering
  \epsfig{file=plots/hpc-cloud.eps, width=\columnwidth}
  \caption{FIXME}
  \label{fig-ppx-cpu}
\end{figure}

\subsection{Cluster Communication Efficiency}

\subsection{Convergence of Large Datasets}
\begin{figure*}[t] % [htb]
  \centering
  \epsfig{file=plots/ppx.eps, width=\textwidth}
  \caption{FIXME}
  \label{fig-ppx-cpu}
\end{figure*}

